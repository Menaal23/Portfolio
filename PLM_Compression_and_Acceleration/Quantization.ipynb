{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_we9RtJTIgNp"
      },
      "source": [
        "# 1. Setup\n",
        "## 1.1 Install PyTorch and HuggingFace Transformers\n",
        "\n",
        "\n",
        "To start this tutorial, let’s first follow the installation instructions in PyTorch [here](https://github.com/pytorch/pytorch/#installation) and HuggingFace Github Repo [here](https://github.com/huggingface/transformers#installation). In addition, we also install [scikit-learn](https://github.com/scikit-learn/scikit-learn) package, as we will reuse its built-in F1 score calculation helper function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKRaaq8FIm3o",
        "outputId": "23a000bc-43f5-4a62-b85a-68f8481aa487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Getting requirements to build wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [15 lines of output]\n",
            "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
            "      rather than 'sklearn' for pip commands.\n",
            "      \n",
            "      Here is how to fix this error in the main use cases:\n",
            "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
            "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
            "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
            "      - if the 'sklearn' package is used by one of your dependencies,\n",
            "        it would be great if you take some time to track which package uses\n",
            "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
            "      - as a last resort, set the environment variable\n",
            "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
            "      \n",
            "      More information is available at\n",
            "      https://github.com/scikit-learn/sklearn-pypi-package\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\n",
            "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "error: subprocess-exited-with-error\n",
            "\n",
            "× Getting requirements to build wheel did not run successfully.\n",
            "│ exit code: 1\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_y6e6cYE-uQ"
      },
      "source": [
        "Because we will be using the experimental parts of the PyTorch, it is recommended to install the latest version of torch and torchvision. You can find the most recent instructions on local installation [here](https://pytorch.org/get-started/locally/). For example, to install on Mac:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNNhTJCRFFSh",
        "outputId": "9b9fd3f8-5d67-40cd-c75f-72d51bb05548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Would remove:\n",
            "    /usr/local/bin/torchfrtrace\n",
            "    /usr/local/bin/torchrun\n",
            "    /usr/local/lib/python3.11/dist-packages/functorch/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torch-2.6.0+cu124.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torch/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torchgen/*\n",
            "Proceed (Y/n)?   Successfully uninstalled torch-2.6.0+cu124\n",
            "y\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision-0.21.0+cu124.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libcudart.41118559.so.12\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libjpeg.1c1c4b09.so.8\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libnvjpeg.02b6d700.so.12\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libpng16.0364a1db.so.16\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libsharpyuv.5c41a003.so.0\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libwebp.54a0d02a.so.7\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision.libs/libz.d13a2644.so.1\n",
            "    /usr/local/lib/python3.11/dist-packages/torchvision/*\n",
            "Proceed (Y/n)?   Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\n",
            "Looking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
            "Collecting torch\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "timm 1.0.15 requires torchvision, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.6.0\n"
          ]
        }
      ],
      "source": [
        "!yes y | pip uninstall torch torchvision\n",
        "!yes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnVXDp3zIgNq"
      },
      "source": [
        "## 1.2 Import the necessary modules\n",
        "\n",
        "In this step we import the necessary Python modules for the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQq5QmXLIgNr",
        "outputId": "d162adb5-37f2-4d32-8bb2-e8b1d84da3f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from argparse import Namespace\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from tqdm import tqdm\n",
        "from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\n",
        "from transformers import glue_compute_metrics as compute_metrics\n",
        "from transformers import glue_output_modes as output_modes\n",
        "from transformers import glue_processors as processors\n",
        "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
        "\n",
        "# Setup logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.WARN)\n",
        "\n",
        "logging.getLogger(\"transformers.modeling_utils\").setLevel(\n",
        "   logging.WARN)  # Reduce logging\n",
        "\n",
        "print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIHLm9FrDuIQ"
      },
      "source": [
        "We set the number of threads to compare the single thread performance between FP32 and INT8 performance. In the end of the tutorial, the user can set other number of threads by building PyTorch with right parallel backend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmhM_lshDvym",
        "outputId": "b7a521e8-4e6b-4a45-b705-ddaf64c20969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ATen/Parallel:\n",
            "\tat::get_num_threads() : 1\n",
            "\tat::get_num_interop_threads() : 1\n",
            "OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "\tomp_get_max_threads() : 1\n",
            "Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications\n",
            "\tmkl_get_max_threads() : 1\n",
            "Intel(R) MKL-DNN v3.5.3 (Git Hash 66f0cb9eb66affd2da3bf5f8d897376f04aae6af)\n",
            "std::thread::hardware_concurrency() : 2\n",
            "Environment variables:\n",
            "\tOMP_NUM_THREADS : [not set]\n",
            "\tMKL_NUM_THREADS : [not set]\n",
            "ATen parallel backend: OpenMP\n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.set_num_threads(1)\n",
        "print(torch.__config__.parallel_info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts3HzlSrIgNx"
      },
      "source": [
        "## 1.3 helper functions\n",
        "\n",
        "The helper functions are built-in in transformers library. We mainly use\n",
        "the following helper functions: one for converting the text examples\n",
        "into the feature vectors; The other one for measuring the F1 score of\n",
        "the predicted result.\n",
        "\n",
        "The [glue_convert_examples_to_features](https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py) function converts the texts into input features:\n",
        "\n",
        "-  Tokenize the input sequences;\n",
        "-  Insert [CLS] in the beginning;\n",
        "-  Insert [SEP] between the first sentence and the second sentence, and\n",
        "   in the end;\n",
        "-  Generate token type ids to indicate whether a token belongs to the\n",
        "   first sequence or the second sequence.\n",
        "\n",
        "The [glue_compute_metrics function](https://github.com/huggingface/transformers/blob/master/transformers/data/processors/glue.py) has the compute metrics with the [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), which\n",
        "can be interpreted as a weighted average of the precision and recall,\n",
        "where an F1 score reaches its best value at 1 and worst score at 0. The\n",
        "relative contribution of precision and recall to the F1 score are equal.\n",
        "\n",
        "-  The equation for the F1 score is:\n",
        "\n",
        "F1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySc1BfWdIgNu"
      },
      "source": [
        "## 1.4 Download the dataset\n",
        "\n",
        "Before running MRPC tasks we download the [GLUE data](https://gluebenchmark.com/tasks) by running [this script](https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e) and unpack it to some directory `glue_data/MRPC`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sw6D1aSHIgNv",
        "outputId": "1786f29e-dcae-4422-84d0-dddc4b4322ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "sample_data\n",
            "--2025-04-18 06:19:52--  https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8225 (8.0K) [text/plain]\n",
            "Saving to: ‘download_glue_data.py’\n",
            "\n",
            "download_glue_data. 100%[===================>]   8.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-18 06:19:53 (89.1 MB/s) - ‘download_glue_data.py’ saved [8225/8225]\n",
            "\n",
            "Processing MRPC...\n",
            "Local MRPC data not specified, downloading data from https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt\n",
            "\tCompleted!\n",
            "dev_ids.tsv  msr_paraphrase_test.txt   test.tsv\n",
            "dev.tsv      msr_paraphrase_train.txt  train.tsv\n"
          ]
        }
      ],
      "source": [
        "# !python download_glue_data.py --data_dir='glue_data' --tasks='MRPC' --test_labels=True\n",
        "!pwd\n",
        "!ls\n",
        "!wget https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\n",
        "!python download_glue_data.py --data_dir='glue_data' --tasks='MRPC'\n",
        "!ls glue_data/MRPC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-y492FMyIgN0"
      },
      "source": [
        "# 2 Fine-tune the BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eFL0mmvrSwX"
      },
      "source": [
        "The spirit of BERT is to pre-train the language representations and then to fine-tune the deep bi-directional representations on a wide range of tasks with minimal task-dependent parameters, and achieves state-of-the-art results. In this tutorial, we will focus on fine-tuning with the pre-trained BERT model to classify semantically equivalent sentence pairs on MRPC task.\n",
        "\n",
        "To fine-tune the pre-trained BERT model (\"bert-base-uncased\" model in HuggingFace transformers) for the MRPC task, you can follow the command in [examples](https://github.com/huggingface/transformers/tree/master/examples):\n",
        "    \n",
        "    export GLUE_DIR=./glue_data\n",
        "    export TASK_NAME=MRPC\n",
        "    export OUT_DIR=./$TASK_NAME/\n",
        "    python ./run_glue.py \\\n",
        "        --model_type bert \\\n",
        "        --model_name_or_path bert-base-uncased \\\n",
        "        --task_name $TASK_NAME \\\n",
        "        --do_train \\\n",
        "        --do_eval \\\n",
        "        --do_lower_case \\\n",
        "        --data_dir $GLUE_DIR/$TASK_NAME \\\n",
        "        --max_seq_length 128 \\\n",
        "        --per_gpu_eval_batch_size=8   \\\n",
        "        --per_gpu_train_batch_size=8   \\\n",
        "        --learning_rate 2e-5 \\\n",
        "        --num_train_epochs 3.0 \\\n",
        "        --save_steps 100000 \\\n",
        "        --output_dir $OUT_DIR\n",
        "\n",
        "\n",
        "We provide the fined-tuned BERT model for MRPC task [here](https://download.pytorch.org/tutorial/MRPC.zip).\n",
        "To save time, you can download the model file (~400 MB) directly into your local folder ``$OUT_DIR``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpBpHtjFaqXC",
        "outputId": "d1a4d088-50b5-4be5-aafc-6258778fbda2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-18 06:19:59--  https://download.pytorch.org/tutorial/MRPC.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 3.169.137.77, 3.169.137.100, 3.169.137.102, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|3.169.137.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 405365618 (387M) [application/zip]\n",
            "Saving to: ‘MRPC.zip’\n",
            "\n",
            "MRPC.zip            100%[===================>] 386.59M  25.4MB/s    in 16s     \n",
            "\n",
            "2025-04-18 06:20:16 (24.3 MB/s) - ‘MRPC.zip’ saved [405365618/405365618]\n",
            "\n",
            "Archive:  MRPC.zip\n",
            "   creating: MRPC/\n",
            " extracting: MRPC/added_tokens.json  \n",
            "  inflating: MRPC/tokenizer_config.json  \n",
            "  inflating: MRPC/special_tokens_map.json  \n",
            "  inflating: MRPC/config.json        \n",
            "  inflating: MRPC/training_args.bin  \n",
            "  inflating: MRPC/vocab.txt          \n",
            "  inflating: MRPC/pytorch_model.bin  \n",
            "download_glue_data.py  glue_data  MRPC\tMRPC.zip  sample_data\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!wget https://download.pytorch.org/tutorial/MRPC.zip\n",
        "!unzip MRPC.zip\n",
        "!ls\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Kuew6wtipv"
      },
      "source": [
        "## 2.1 Set global configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O69tatmbtnJ3"
      },
      "source": [
        "Here we set the global configurations for evaluating the fine-tuned BERT model before and after the dynamic quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-ZdUEGOIgN0",
        "outputId": "61cf5385-28ea-447b-c44f-78ddae119e00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/processors/glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "configs = Namespace()\n",
        "\n",
        "# The output directory for the fine-tuned model.\n",
        "configs.output_dir = \"/content/MRPC/\"\n",
        "# configs.output_dir = \"./MRPC/\"\n",
        "\n",
        "# The data directory for the MRPC task in the GLUE benchmark.\n",
        "configs.data_dir = \"/content/glue_data/MRPC\"\n",
        "\n",
        "# The model name or path for the pre-trained model.\n",
        "configs.model_name_or_path = \"bert-base-uncased\"\n",
        "# The maximum length of an input sequence\n",
        "configs.max_seq_length = 128\n",
        "\n",
        "# Prepare GLUE task.\n",
        "configs.task_name = \"MRPC\".lower()\n",
        "configs.processor = processors[configs.task_name]()\n",
        "configs.output_mode = output_modes[configs.task_name]\n",
        "configs.label_list = configs.processor.get_labels()\n",
        "configs.model_type = \"bert\".lower()\n",
        "configs.do_lower_case = True\n",
        "\n",
        "# Set the device, batch size, topology, and caching flags.\n",
        "configs.device = \"cpu\"\n",
        "configs.per_gpu_eval_batch_size = 8\n",
        "configs.n_gpu = 0\n",
        "configs.local_rank = -1\n",
        "configs.overwrite_cache = False\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility.\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVj1KXhOIgN4"
      },
      "source": [
        "## 2.2 Load the fine-tuned BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO6tENlSo9Pl"
      },
      "source": [
        "We load the tokenizer and fine-tuned BERT sequence classifier model (FP32) from the `configs.output_dir`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzyVSIKYIgN5",
        "outputId": "de9126ec-79ef-4edf-cfb7-f33d205f4c06"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    configs.output_dir, do_lower_case=configs.do_lower_case)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(configs.output_dir)\n",
        "model.to(configs.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEQei5wkroBH"
      },
      "source": [
        "## 2.3 Define the tokenize and evaluation function\n",
        "We reuse the tokenize and evaluation function from [HuggingFace](https://github.com/huggingface/transformers/blob/master/examples/run_glue.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fJsPaJXMIgN3"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n",
        "    eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,)\n",
        "\n",
        "    results = {}\n",
        "    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n",
        "        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n",
        "\n",
        "        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "            os.makedirs(eval_output_dir)\n",
        "\n",
        "        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "        # Note that DistributedSampler samples randomly\n",
        "        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        # multi-gpu eval\n",
        "        if args.n_gpu > 1:\n",
        "            model = torch.nn.DataParallel(model)\n",
        "\n",
        "        # Eval!\n",
        "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "        eval_loss = 0.0\n",
        "        nb_eval_steps = 0\n",
        "        preds = None\n",
        "        out_label_ids = None\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            model.eval()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = {'input_ids':      batch[0],\n",
        "                          'attention_mask': batch[1],\n",
        "                          'labels':         batch[3]}\n",
        "                if args.model_type != 'distilbert':\n",
        "                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n",
        "                outputs = model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "                eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "                out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        if args.output_mode == \"classification\":\n",
        "            preds = np.argmax(preds, axis=1)\n",
        "        elif args.output_mode == \"regression\":\n",
        "            preds = np.squeeze(preds)\n",
        "        result = compute_metrics(eval_task, preds, out_label_ids)\n",
        "        results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as writer:\n",
        "            logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "            for key in sorted(result.keys()):\n",
        "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n",
        "    if args.local_rank not in [-1, 0] and not evaluate:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "    processor = processors[task]()\n",
        "    output_mode = output_modes[task]\n",
        "    # Load data features from cache or dataset file\n",
        "    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n",
        "        'dev' if evaluate else 'train',\n",
        "        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n",
        "        str(args.max_seq_length),\n",
        "        str(task)))\n",
        "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        label_list = processor.get_labels()\n",
        "        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta']:\n",
        "            # HACK(label indices are swapped in RoBERTa pretrained model)\n",
        "            label_list[1], label_list[2] = label_list[2], label_list[1]\n",
        "        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n",
        "        features = convert_examples_to_features(examples,\n",
        "                                                tokenizer,\n",
        "                                                label_list=label_list,\n",
        "                                                max_length=args.max_seq_length,\n",
        "                                                output_mode=output_mode\n",
        "        )\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            torch.save(features, cached_features_file)\n",
        "\n",
        "    if args.local_rank == 0 and not evaluate:\n",
        "        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    if output_mode == \"classification\":\n",
        "        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    elif output_mode == \"regression\":\n",
        "        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWQ6E5z6IgN7"
      },
      "source": [
        "# 3. Apply the dynamic quantization\n",
        "\n",
        "We call `torch.quantization.quantize_dynamic` on the model to apply the dynamic quantization on the HuggingFace BERT model. Specifically,\n",
        "\n",
        "- We specify that we want the torch.nn.Linear modules in our model to be quantized;\n",
        "- We specify that we want weights to be converted to quantized int8 values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnMpkij7IgN7",
        "outputId": "e435afd9-b933-4fe9-935b-7313fe14e49d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BertForSequenceClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSdpaSelfAttention(\n",
            "              (query): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "              (key): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "              (value): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): DynamicQuantizedLinear(in_features=768, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")\n",
        "print(quantized_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXAb1bHcIgN-"
      },
      "source": [
        "## 3.1 Check the model size\n",
        "Let's first check the model size. We can observe a significant reduction in model size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sTUmFJfIgN-",
        "outputId": "3706018c-7e3b-4eee-9738-6974a70777cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size (MB): 438.000505\n",
            "Size (MB): 181.479765\n"
          ]
        }
      ],
      "source": [
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
        "    os.remove('temp.p')\n",
        "\n",
        "print_size_of_model(model)\n",
        "print_size_of_model(quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ0esT_qIgOC"
      },
      "source": [
        "The BERT model used in this tutorial (bert-base-uncased) has a vocabulary size V of 30522. With the embedding size of 768, the total size of the word embedding table is ~ 4 (Bytes/FP32) * 30522 * 768 = 90 MB. So with the help of quantization, the model size of the non-embedding table part is reduced from 350 MB (FP32 model) to 90 MB (INT8 model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Laj0oGh4ifxu"
      },
      "source": [
        "## 3.2 Evaluate the inference accuracy and time\n",
        "\n",
        "Next, let's compare the inference time as well as the evaluation accuracy between the original FP32 model and the INT8 model after the dynamic quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foe-dVxHIgOC",
        "outputId": "d1ed6b99-850a-40dc-dbd6-088c0a8da44b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/processors/glue.py:66: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n",
            "Evaluating: 100%|██████████| 51/51 [02:37<00:00,  3.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'acc': np.float64(0.8602941176470589), 'f1': 0.9018932874354562, 'acc_and_f1': np.float64(0.8810937025412575)}\n",
            "Evaluate total time (seconds): 157.9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "def time_model_evaluation(model, configs, tokenizer):\n",
        "    eval_start_time = time.time()\n",
        "    result = evaluate(configs, model, tokenizer, prefix=\"\")\n",
        "    eval_end_time = time.time()\n",
        "    eval_duration_time = eval_end_time - eval_start_time\n",
        "    print(result)\n",
        "    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n",
        "\n",
        "# Evaluate the original FP32 BERT model\n",
        "time_model_evaluation(model, configs, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oOb7iTBUhCMS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers.data.processors.utils import InputFeatures\n",
        "\n",
        "torch.serialization.add_safe_globals([InputFeatures])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yct4a6SKk0ai",
        "outputId": "1147dac5-ce82-43a9-f7b3-36388ebf7d94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/processors/glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
            "Evaluating: 100%|██████████| 51/51 [01:53<00:00,  2.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'acc': np.float64(0.8553921568627451), 'f1': 0.8977469670710572, 'acc_and_f1': np.float64(0.8765695619669012)}\n",
            "Evaluate total time (seconds): 113.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the INT8 BERT model after the dynamic quantization\n",
        "time_model_evaluation(quantized_model, configs, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hpElZt2Pi-kT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def evaluate_model_with_threads(model, configs, tokenizer, num_threads):\n",
        "    torch.set_num_threads(num_threads)  # set the number of threads\n",
        "    eval_start_time = time.time()\n",
        "    result = evaluate(configs, model, tokenizer, prefix=\"\")\n",
        "    eval_end_time = time.time()\n",
        "    eval_duration_time = eval_end_time - eval_start_time\n",
        "    f1_score = result[\"f1\"] if \"f1\" in result else None\n",
        "    return f1_score, eval_duration_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FFWR2UuBjJo5"
      },
      "outputs": [],
      "source": [
        "def get_model_size(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size_mb = os.path.getsize(\"temp.p\")/1e6\n",
        "    os.remove('temp.p')\n",
        "    return size_mb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMQBkpRojL7j",
        "outputId": "465ca4a4-4e0a-4ff7-df9c-39d5d695375a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/processors/glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
            "Evaluating: 100%|██████████| 51/51 [02:34<00:00,  3.03s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/processors/glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
            "Evaluating: 100%|██████████| 51/51 [02:36<00:00,  3.06s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/processors/glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
            "Evaluating: 100%|██████████| 51/51 [01:52<00:00,  2.21s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/processors/glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
            "Evaluating: 100%|██████████| 51/51 [01:48<00:00,  2.13s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Evaluate FP32 model\n",
        "fp32_size = get_model_size(model)\n",
        "fp32_f1_1t, fp32_time_1t = evaluate_model_with_threads(model, configs, tokenizer, num_threads=1)\n",
        "fp32_f1_4t, fp32_time_4t = evaluate_model_with_threads(model, configs, tokenizer, num_threads=4)\n",
        "\n",
        "# Evaluate INT8 model\n",
        "int8_size = get_model_size(quantized_model)\n",
        "int8_f1_1t, int8_time_1t = evaluate_model_with_threads(quantized_model, configs, tokenizer, num_threads=1)\n",
        "int8_f1_4t, int8_time_4t = evaluate_model_with_threads(quantized_model, configs, tokenizer, num_threads=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14NX7-NjjLo6",
        "outputId": "dc3d7658-addc-4b9a-ce91-652bf5c71fff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Prec  F1 score  Model Size (MB)  1 thread (sec)  4 threads (sec)\n",
            "0  FP32    0.9019            438.0             155              156\n",
            "1  INT8    0.8977            181.5             113              109\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Prec\": [\"FP32\", \"INT8\"],\n",
        "    \"F1 score\": [round(fp32_f1_1t, 4), round(int8_f1_1t, 4)],  # or avg f1 between 1t and 4t\n",
        "    \"Model Size (MB)\": [round(fp32_size, 1), round(int8_size, 1)],\n",
        "    \"1 thread (sec)\": [round(fp32_time_1t), round(int8_time_1t)],\n",
        "    \"4 threads (sec)\": [round(fp32_time_4t), round(int8_time_4t)]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdMd7b5aIgOE",
        "outputId": "1d431425-fe66-46f5-b95e-f2246dd820c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/processors/glue.py:174: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
            "Evaluating: 100%|██████████| 51/51 [02:04<00:00,  2.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'acc': np.float64(0.8553921568627451), 'f1': 0.8977469670710572, 'acc_and_f1': np.float64(0.8765695619669012)}\n",
            "Evaluate total time (seconds): 124.7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:37: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
            "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the INT8 BERT model after the dynamic quantization\n",
        "time_model_evaluation(quantized_model, configs, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "KNIjvg9NIgOI"
      },
      "source": [
        "Running this locally on a MacBook Pro, without quantization, inference (for all 408 examples in MRPC dataset) takes about 160 seconds, and with quantization it takes just about 90 seconds. We summarize the results for running the quantized BERT model inference on a Macbook Pro as the follows:\n",
        "\n",
        "```\n",
        "| Prec | F1 score | Model Size | 1 thread | 4 threads |\n",
        "| FP32 |  0.9019  |   438 MB   | 160 sec  | 85 sec    |\n",
        "| INT8 |  0.8953  |   181 MB   |  90 sec  | 46 sec    |\n",
        "```\n",
        "\n",
        "We have 0.6% F1 score accuracy after applying the post-training dynamic quantization on the fine-tuned BERT model on the MRPC task. As a comparison, in a [recent paper](https://arxiv.org/pdf/1910.06188.pdf) (Table 1), it achieved 0.8788 by applying the post-training dynamic quantization and 0.8956 by applying the quantization-aware training. The main difference is that we support the asymmetric quantization in PyTorch while that paper supports the symmetric quantization only.\n",
        "\n",
        "Note that we set the number of threads to 1 for the single-thread comparison in this tutorial. We also support the intra-op parallelization for these quantized INT8 operators. The users can now set multi-thread by `torch.set_num_threads(N)` (`N` is the number of intra-op parallelization threads). One preliminary requirement to enable the intra-op parallelization support is to build PyTorch with the right [backend](https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html#build-options) such as OpenMP, Native, or TBB. You can use `torch.__config__.parallel_info()` to check the parallelization settings. On the same MacBook Pro using PyTorch with Native backend for parallelization, we can get about 46 seconds for processing the evaluation of MRPC dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTQDJ8Y1syVm"
      },
      "source": [
        "## 3.3 Serialize the quantized model\n",
        "We can serialize and save the quantized model for the future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZJH-Ln0IgOG"
      },
      "outputs": [],
      "source": [
        "quantized_output_dir = configs.output_dir + \"quantized/\"\n",
        "if not os.path.exists(quantized_output_dir):\n",
        "    os.makedirs(quantized_output_dir)\n",
        "    quantized_model.save_pretrained(quantized_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMKPDl8rIgOJ"
      },
      "source": [
        "# Conclusion\n",
        "In this tutorial, we demonstrated how to demonstrate how to convert a well-known state-of-the-art NLP model like BERT into dynamic quantized model. Dynamic quantization can reduce the size of the model while only having a limited implication on accuracy.\n",
        "\n",
        "Thanks for reading! As always, we welcome any feedback, so please create an issue [here](https://github.com/pytorch/pytorch/issues) if you have any."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfnTjseZdnvu"
      },
      "source": [
        "# References\n",
        "[1] J.Devlin, M. Chang, K. Lee and K. Toutanova, [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) (2018)\n",
        "\n",
        "[2] [HuggingFace Transformers](https://github.com/huggingface/transformers).\n",
        "\n",
        "[3] O. Zafrir, G. Boudoukh, P. Izsak, & M. Wasserblat (2019). [Q8BERT: Quantized 8bit BERT](https://arxiv.org/pdf/1910.06188.pdf).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "g049cwB-IgOJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IywQkvCIgOK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anp_cloned_from": {
      "notebook_id": "700375763812832",
      "revision_id": "390006458366428"
    },
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "colab": {
      "provenance": []
    },
    "disseminate_notebook_id": {
      "notebook_id": "736675493480417"
    },
    "disseminate_notebook_info": {
      "bento_version": "20191111-000215",
      "description": "BERT tutorial for OSS",
      "hide_code": false,
      "hipster_group": "",
      "kernel_build_info": {
        "deps": [
          "//aml/integrity_solutions/bento:library"
        ],
        "external_deps": []
      },
      "no_uii": true,
      "notebook_number": "173786",
      "others_can_edit": false,
      "reviewers": "",
      "revision_id": "2665524713486279",
      "tags": "",
      "tasks": "",
      "title": "Dynamic Quantization on HuggingFace BERT model"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
