{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers peft accelerate bitsandbytes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:16:10.202477Z","iopub.execute_input":"2025-04-18T14:16:10.202692Z","iopub.status.idle":"2025-04-18T14:17:40.102890Z","shell.execute_reply.started":"2025-04-18T14:16:10.202669Z","shell.execute_reply":"2025-04-18T14:17:40.102111Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cu124)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:18:23.362633Z","iopub.execute_input":"2025-04-18T14:18:23.363251Z","iopub.status.idle":"2025-04-18T14:18:27.106299Z","shell.execute_reply.started":"2025-04-18T14:18:23.363224Z","shell.execute_reply":"2025-04-18T14:18:27.105601Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:18:33.471317Z","iopub.execute_input":"2025-04-18T14:18:33.472176Z","iopub.status.idle":"2025-04-18T14:19:08.308397Z","shell.execute_reply.started":"2025-04-18T14:18:33.472146Z","shell.execute_reply":"2025-04-18T14:19:08.307824Z"}},"outputs":[{"name":"stderr","text":"2025-04-18 14:18:51.956107: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744985932.363813      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744985932.489853      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model_name = \"bert-base-uncased\"\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:19:08.309558Z","iopub.execute_input":"2025-04-18T14:19:08.310161Z","iopub.status.idle":"2025-04-18T14:19:13.306220Z","shell.execute_reply.started":"2025-04-18T14:19:08.310140Z","shell.execute_reply":"2025-04-18T14:19:13.305653Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c0131feadae4d10981a2e6a68734f1c"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adeff6e4471b46b78b5e653262758446"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d432b70e99aa4f07925d738f1363eb22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7aa687329be494a94f6e9634cd792c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb7244b07eea408aa0560c45152ce4a8"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"config = LoraConfig(\n    r=8,                    # Rank of the low-rank update\n    lora_alpha=32,           # Scaling factor\n    target_modules=[\"query\", \"value\"],  # Compress only attention Q and V\n    lora_dropout=0.1,        # Dropout inside LoRA\n    bias=\"none\",             # No bias terms\n    task_type=\"SEQ_CLS\"      # Sequence classification task\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:20:10.837382Z","iopub.execute_input":"2025-04-18T14:20:10.837654Z","iopub.status.idle":"2025-04-18T14:20:10.841703Z","shell.execute_reply.started":"2025-04-18T14:20:10.837635Z","shell.execute_reply":"2025-04-18T14:20:10.841113Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"compressed_model = get_peft_model(model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:21:54.276789Z","iopub.execute_input":"2025-04-18T14:21:54.277193Z","iopub.status.idle":"2025-04-18T14:21:54.305221Z","shell.execute_reply.started":"2025-04-18T14:21:54.277165Z","shell.execute_reply":"2025-04-18T14:21:54.304487Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"compressed_model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:21:58.148927Z","iopub.execute_input":"2025-04-18T14:21:58.149210Z","iopub.status.idle":"2025-04-18T14:21:58.155165Z","shell.execute_reply.started":"2025-04-18T14:21:58.149192Z","shell.execute_reply":"2025-04-18T14:21:58.154269Z"}},"outputs":[{"name":"stdout","text":"trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"sentence = \"The movie was amazing!\"\n\ninputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True)\nlabels = torch.tensor([1]).unsqueeze(0)  # Example label (e.g., 1 = positive sentiment)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:22:02.222070Z","iopub.execute_input":"2025-04-18T14:22:02.222620Z","iopub.status.idle":"2025-04-18T14:22:02.236987Z","shell.execute_reply.started":"2025-04-18T14:22:02.222600Z","shell.execute_reply":"2025-04-18T14:22:02.236353Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"compressed_model.eval()\nwith torch.no_grad():\n    outputs = compressed_model(**inputs, labels=labels)\n    loss = outputs.loss\n    logits = outputs.logits\n\n    preds = torch.argmax(logits, dim=-1)\n\nprint(f\"Predicted class: {preds.item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:22:07.267427Z","iopub.execute_input":"2025-04-18T14:22:07.267677Z","iopub.status.idle":"2025-04-18T14:22:07.526578Z","shell.execute_reply.started":"2025-04-18T14:22:07.267660Z","shell.execute_reply":"2025-04-18T14:22:07.525983Z"}},"outputs":[{"name":"stdout","text":"Predicted class: 1\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"**Fine-Tuning with LoRA and BERT Base on GLUE-SST**","metadata":{}},{"cell_type":"markdown","source":"**Load the SST-2 Dataset**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the SST-2 dataset\ndataset = load_dataset(\"glue\", \"sst2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:22:25.154996Z","iopub.execute_input":"2025-04-18T14:22:25.155690Z","iopub.status.idle":"2025-04-18T14:22:35.996621Z","shell.execute_reply.started":"2025-04-18T14:22:25.155656Z","shell.execute_reply":"2025-04-18T14:22:35.995937Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2410b73ae964f5eb784640c7e7ba11a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59bcbb3b6e104afab0737f174b02a620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3ef71d8d8284734a6ef615a6b725268"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9c27929688145cca851908aa0e959e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"637aff2bebad4d7488c7fefd4c56e702"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c04069216f7b4d0b9824933372326a9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4948596d8edf4a378b931f0d81a92d12"}},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"**Preprocessing the Data**","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load tokenizer for BERT\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef preprocess_function(examples):\n    return tokenizer(examples['sentence'], padding=\"max_length\", truncation=True)\n\n# Apply the tokenizer\nencoded_dataset = dataset.map(preprocess_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:23:16.484243Z","iopub.execute_input":"2025-04-18T14:23:16.485097Z","iopub.status.idle":"2025-04-18T14:23:36.978103Z","shell.execute_reply.started":"2025-04-18T14:23:16.485063Z","shell.execute_reply":"2025-04-18T14:23:36.977511Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"522bde37bcea4c0989ddd94a939c2031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b59abdbf20141869c7d227db095b0cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ff523fe935b4120af5bbd2b26dcc17a"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"pip install wandb\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:24:21.248116Z","iopub.execute_input":"2025-04-18T14:24:21.248378Z","iopub.status.idle":"2025-04-18T14:24:24.255301Z","shell.execute_reply.started":"2025-04-18T14:24:21.248361Z","shell.execute_reply":"2025-04-18T14:24:24.254270Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"1021378ec87a0c08f4bf0d26ccad6cd8c2813bf8\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:24:35.608915Z","iopub.execute_input":"2025-04-18T14:24:35.609552Z","iopub.status.idle":"2025-04-18T14:24:44.078282Z","shell.execute_reply.started":"2025-04-18T14:24:35.609522Z","shell.execute_reply":"2025-04-18T14:24:44.077692Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshehrosemir2000\u001b[0m (\u001b[33mshehrosemir2000-fast-nuces\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"**Fine-Tune LoRA Model**","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model\nimport torch\nfrom sklearn.metrics import accuracy_score\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n    acc = accuracy_score(labels, predictions)\n    return {\"accuracy\": acc}\n\n# Load BERT model\nmodel_name = \"bert-base-uncased\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n# Define LoRA config\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"query\", \"value\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"SEQ_CLS\"\n)\n\n# Apply LoRA to the model\ncompressed_model = get_peft_model(model, config)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    do_train=True,\n    do_eval=True,\n    eval_steps=500,  # Evaluate after every 500 steps\n    \n)\n\n\n\n# Set up Trainer\ntrainer = Trainer(\n    model=compressed_model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# Fine-tune the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T14:27:21.959802Z","iopub.execute_input":"2025-04-18T14:27:21.960638Z","iopub.status.idle":"2025-04-18T16:10:27.938580Z","shell.execute_reply.started":"2025-04-18T14:27:21.960612Z","shell.execute_reply":"2025-04-18T16:10:27.938059Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_31/2063448828.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250418_142725-zqwmkrrp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shehrosemir2000-fast-nuces/huggingface/runs/zqwmkrrp' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/shehrosemir2000-fast-nuces/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shehrosemir2000-fast-nuces/huggingface' target=\"_blank\">https://wandb.ai/shehrosemir2000-fast-nuces/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shehrosemir2000-fast-nuces/huggingface/runs/zqwmkrrp' target=\"_blank\">https://wandb.ai/shehrosemir2000-fast-nuces/huggingface/runs/zqwmkrrp</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4210' max='4210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4210/4210 1:42:50, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.724100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.704000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.696800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.688900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.685900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.675500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.689200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.689700</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.688800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.684100</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.675300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.674000</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.688900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.684000</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.696200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.675300</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.682800</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.679200</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.689100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.661500</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.670000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.677600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.663500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.675600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.656100</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.658700</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.652100</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.677700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.654500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.624800</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.651400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.627600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.628000</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.581800</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.574600</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.569300</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.576200</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.527800</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.509400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.497600</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.474000</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.447200</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.444100</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.403500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.461700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.363200</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.420700</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.346100</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.365700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.375200</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.346600</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.322300</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.411400</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.308700</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.350000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.346300</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.365400</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.337500</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.325000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.312800</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.387100</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.351400</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.354000</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.365900</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.353700</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.299900</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.355700</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.312700</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.279300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.360200</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.365500</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.324000</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.270300</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.423700</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.344600</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.298500</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.310300</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.358200</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.283500</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.323200</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.301100</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.347000</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.316900</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.336300</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.294800</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.252300</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.341300</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.340700</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.289400</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.355100</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.346300</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.325100</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.337200</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.322200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.294500</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.304400</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.325900</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.272700</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.310100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.333400</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.380600</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.379300</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.338000</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.318600</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.315800</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.315500</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.340000</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.298100</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.254900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.342000</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.327600</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.305400</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.275200</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.286000</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.247700</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.332200</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.336600</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.274900</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.285600</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.342500</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.253500</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.340000</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.290000</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.305100</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.250200</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.289800</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.298000</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.349800</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.284100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.335900</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.306500</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.346300</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.292700</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.301600</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.285600</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.367000</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.293900</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.318000</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.300400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.305500</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.318400</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.327400</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.333600</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.314600</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.259900</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.358300</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.311500</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.285400</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.290300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.334800</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.297400</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.284600</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.328200</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.263700</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.311300</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.330500</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.312500</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.363800</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.336200</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.310600</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.212800</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.317000</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.265800</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.290800</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.306000</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.319900</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.250600</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.282900</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.319200</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.343200</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.272100</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.318700</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.307600</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.305900</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.256000</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.351000</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.311200</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.308700</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.246400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.306800</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.344800</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.273000</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.251200</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.254300</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.281000</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.354300</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.254600</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.293100</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.292900</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.260500</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.297800</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.281700</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.276600</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.292900</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.237000</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.275000</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.290800</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.313600</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.270000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.298400</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.233200</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.296700</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.257500</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.306100</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.270300</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.298100</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.306500</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.279800</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.297800</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.341700</td>\n    </tr>\n    <tr>\n      <td>2110</td>\n      <td>0.324000</td>\n    </tr>\n    <tr>\n      <td>2120</td>\n      <td>0.313900</td>\n    </tr>\n    <tr>\n      <td>2130</td>\n      <td>0.268700</td>\n    </tr>\n    <tr>\n      <td>2140</td>\n      <td>0.295000</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>0.309400</td>\n    </tr>\n    <tr>\n      <td>2160</td>\n      <td>0.276700</td>\n    </tr>\n    <tr>\n      <td>2170</td>\n      <td>0.283300</td>\n    </tr>\n    <tr>\n      <td>2180</td>\n      <td>0.277700</td>\n    </tr>\n    <tr>\n      <td>2190</td>\n      <td>0.323800</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.304500</td>\n    </tr>\n    <tr>\n      <td>2210</td>\n      <td>0.290900</td>\n    </tr>\n    <tr>\n      <td>2220</td>\n      <td>0.244400</td>\n    </tr>\n    <tr>\n      <td>2230</td>\n      <td>0.293600</td>\n    </tr>\n    <tr>\n      <td>2240</td>\n      <td>0.263800</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>0.277100</td>\n    </tr>\n    <tr>\n      <td>2260</td>\n      <td>0.335600</td>\n    </tr>\n    <tr>\n      <td>2270</td>\n      <td>0.261800</td>\n    </tr>\n    <tr>\n      <td>2280</td>\n      <td>0.327500</td>\n    </tr>\n    <tr>\n      <td>2290</td>\n      <td>0.250700</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.296000</td>\n    </tr>\n    <tr>\n      <td>2310</td>\n      <td>0.263300</td>\n    </tr>\n    <tr>\n      <td>2320</td>\n      <td>0.246100</td>\n    </tr>\n    <tr>\n      <td>2330</td>\n      <td>0.306400</td>\n    </tr>\n    <tr>\n      <td>2340</td>\n      <td>0.224600</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>0.274300</td>\n    </tr>\n    <tr>\n      <td>2360</td>\n      <td>0.256900</td>\n    </tr>\n    <tr>\n      <td>2370</td>\n      <td>0.269300</td>\n    </tr>\n    <tr>\n      <td>2380</td>\n      <td>0.333900</td>\n    </tr>\n    <tr>\n      <td>2390</td>\n      <td>0.259800</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.249300</td>\n    </tr>\n    <tr>\n      <td>2410</td>\n      <td>0.316400</td>\n    </tr>\n    <tr>\n      <td>2420</td>\n      <td>0.310200</td>\n    </tr>\n    <tr>\n      <td>2430</td>\n      <td>0.281000</td>\n    </tr>\n    <tr>\n      <td>2440</td>\n      <td>0.311600</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>0.352900</td>\n    </tr>\n    <tr>\n      <td>2460</td>\n      <td>0.274000</td>\n    </tr>\n    <tr>\n      <td>2470</td>\n      <td>0.327100</td>\n    </tr>\n    <tr>\n      <td>2480</td>\n      <td>0.236000</td>\n    </tr>\n    <tr>\n      <td>2490</td>\n      <td>0.315500</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.319400</td>\n    </tr>\n    <tr>\n      <td>2510</td>\n      <td>0.317400</td>\n    </tr>\n    <tr>\n      <td>2520</td>\n      <td>0.302300</td>\n    </tr>\n    <tr>\n      <td>2530</td>\n      <td>0.314800</td>\n    </tr>\n    <tr>\n      <td>2540</td>\n      <td>0.310300</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>0.290800</td>\n    </tr>\n    <tr>\n      <td>2560</td>\n      <td>0.252700</td>\n    </tr>\n    <tr>\n      <td>2570</td>\n      <td>0.246400</td>\n    </tr>\n    <tr>\n      <td>2580</td>\n      <td>0.321600</td>\n    </tr>\n    <tr>\n      <td>2590</td>\n      <td>0.315400</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.302600</td>\n    </tr>\n    <tr>\n      <td>2610</td>\n      <td>0.284100</td>\n    </tr>\n    <tr>\n      <td>2620</td>\n      <td>0.257600</td>\n    </tr>\n    <tr>\n      <td>2630</td>\n      <td>0.305100</td>\n    </tr>\n    <tr>\n      <td>2640</td>\n      <td>0.270000</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>0.299400</td>\n    </tr>\n    <tr>\n      <td>2660</td>\n      <td>0.261300</td>\n    </tr>\n    <tr>\n      <td>2670</td>\n      <td>0.314800</td>\n    </tr>\n    <tr>\n      <td>2680</td>\n      <td>0.306300</td>\n    </tr>\n    <tr>\n      <td>2690</td>\n      <td>0.306000</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.310700</td>\n    </tr>\n    <tr>\n      <td>2710</td>\n      <td>0.252300</td>\n    </tr>\n    <tr>\n      <td>2720</td>\n      <td>0.276000</td>\n    </tr>\n    <tr>\n      <td>2730</td>\n      <td>0.298000</td>\n    </tr>\n    <tr>\n      <td>2740</td>\n      <td>0.305800</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>0.279100</td>\n    </tr>\n    <tr>\n      <td>2760</td>\n      <td>0.332100</td>\n    </tr>\n    <tr>\n      <td>2770</td>\n      <td>0.313700</td>\n    </tr>\n    <tr>\n      <td>2780</td>\n      <td>0.292100</td>\n    </tr>\n    <tr>\n      <td>2790</td>\n      <td>0.271800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.243400</td>\n    </tr>\n    <tr>\n      <td>2810</td>\n      <td>0.281300</td>\n    </tr>\n    <tr>\n      <td>2820</td>\n      <td>0.280900</td>\n    </tr>\n    <tr>\n      <td>2830</td>\n      <td>0.251400</td>\n    </tr>\n    <tr>\n      <td>2840</td>\n      <td>0.241600</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>0.263000</td>\n    </tr>\n    <tr>\n      <td>2860</td>\n      <td>0.232300</td>\n    </tr>\n    <tr>\n      <td>2870</td>\n      <td>0.262400</td>\n    </tr>\n    <tr>\n      <td>2880</td>\n      <td>0.240900</td>\n    </tr>\n    <tr>\n      <td>2890</td>\n      <td>0.273100</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.208300</td>\n    </tr>\n    <tr>\n      <td>2910</td>\n      <td>0.313400</td>\n    </tr>\n    <tr>\n      <td>2920</td>\n      <td>0.239900</td>\n    </tr>\n    <tr>\n      <td>2930</td>\n      <td>0.290300</td>\n    </tr>\n    <tr>\n      <td>2940</td>\n      <td>0.318800</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>0.283100</td>\n    </tr>\n    <tr>\n      <td>2960</td>\n      <td>0.271900</td>\n    </tr>\n    <tr>\n      <td>2970</td>\n      <td>0.265800</td>\n    </tr>\n    <tr>\n      <td>2980</td>\n      <td>0.290900</td>\n    </tr>\n    <tr>\n      <td>2990</td>\n      <td>0.266100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.237300</td>\n    </tr>\n    <tr>\n      <td>3010</td>\n      <td>0.285900</td>\n    </tr>\n    <tr>\n      <td>3020</td>\n      <td>0.307600</td>\n    </tr>\n    <tr>\n      <td>3030</td>\n      <td>0.304400</td>\n    </tr>\n    <tr>\n      <td>3040</td>\n      <td>0.304100</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>0.303100</td>\n    </tr>\n    <tr>\n      <td>3060</td>\n      <td>0.238600</td>\n    </tr>\n    <tr>\n      <td>3070</td>\n      <td>0.264100</td>\n    </tr>\n    <tr>\n      <td>3080</td>\n      <td>0.269100</td>\n    </tr>\n    <tr>\n      <td>3090</td>\n      <td>0.278800</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.248000</td>\n    </tr>\n    <tr>\n      <td>3110</td>\n      <td>0.294200</td>\n    </tr>\n    <tr>\n      <td>3120</td>\n      <td>0.256700</td>\n    </tr>\n    <tr>\n      <td>3130</td>\n      <td>0.254600</td>\n    </tr>\n    <tr>\n      <td>3140</td>\n      <td>0.280400</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>0.259300</td>\n    </tr>\n    <tr>\n      <td>3160</td>\n      <td>0.276000</td>\n    </tr>\n    <tr>\n      <td>3170</td>\n      <td>0.383600</td>\n    </tr>\n    <tr>\n      <td>3180</td>\n      <td>0.255700</td>\n    </tr>\n    <tr>\n      <td>3190</td>\n      <td>0.257900</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.309900</td>\n    </tr>\n    <tr>\n      <td>3210</td>\n      <td>0.317700</td>\n    </tr>\n    <tr>\n      <td>3220</td>\n      <td>0.250600</td>\n    </tr>\n    <tr>\n      <td>3230</td>\n      <td>0.311900</td>\n    </tr>\n    <tr>\n      <td>3240</td>\n      <td>0.340800</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>0.342100</td>\n    </tr>\n    <tr>\n      <td>3260</td>\n      <td>0.236700</td>\n    </tr>\n    <tr>\n      <td>3270</td>\n      <td>0.283900</td>\n    </tr>\n    <tr>\n      <td>3280</td>\n      <td>0.308300</td>\n    </tr>\n    <tr>\n      <td>3290</td>\n      <td>0.208900</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.297800</td>\n    </tr>\n    <tr>\n      <td>3310</td>\n      <td>0.290800</td>\n    </tr>\n    <tr>\n      <td>3320</td>\n      <td>0.270800</td>\n    </tr>\n    <tr>\n      <td>3330</td>\n      <td>0.303900</td>\n    </tr>\n    <tr>\n      <td>3340</td>\n      <td>0.258600</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>0.292400</td>\n    </tr>\n    <tr>\n      <td>3360</td>\n      <td>0.244900</td>\n    </tr>\n    <tr>\n      <td>3370</td>\n      <td>0.337000</td>\n    </tr>\n    <tr>\n      <td>3380</td>\n      <td>0.233400</td>\n    </tr>\n    <tr>\n      <td>3390</td>\n      <td>0.301100</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.298800</td>\n    </tr>\n    <tr>\n      <td>3410</td>\n      <td>0.255900</td>\n    </tr>\n    <tr>\n      <td>3420</td>\n      <td>0.298300</td>\n    </tr>\n    <tr>\n      <td>3430</td>\n      <td>0.299100</td>\n    </tr>\n    <tr>\n      <td>3440</td>\n      <td>0.296500</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>0.315200</td>\n    </tr>\n    <tr>\n      <td>3460</td>\n      <td>0.220800</td>\n    </tr>\n    <tr>\n      <td>3470</td>\n      <td>0.269600</td>\n    </tr>\n    <tr>\n      <td>3480</td>\n      <td>0.265000</td>\n    </tr>\n    <tr>\n      <td>3490</td>\n      <td>0.252700</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.279400</td>\n    </tr>\n    <tr>\n      <td>3510</td>\n      <td>0.231800</td>\n    </tr>\n    <tr>\n      <td>3520</td>\n      <td>0.246600</td>\n    </tr>\n    <tr>\n      <td>3530</td>\n      <td>0.276100</td>\n    </tr>\n    <tr>\n      <td>3540</td>\n      <td>0.264300</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>0.222400</td>\n    </tr>\n    <tr>\n      <td>3560</td>\n      <td>0.280000</td>\n    </tr>\n    <tr>\n      <td>3570</td>\n      <td>0.288600</td>\n    </tr>\n    <tr>\n      <td>3580</td>\n      <td>0.337700</td>\n    </tr>\n    <tr>\n      <td>3590</td>\n      <td>0.287900</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.249400</td>\n    </tr>\n    <tr>\n      <td>3610</td>\n      <td>0.286000</td>\n    </tr>\n    <tr>\n      <td>3620</td>\n      <td>0.316600</td>\n    </tr>\n    <tr>\n      <td>3630</td>\n      <td>0.275500</td>\n    </tr>\n    <tr>\n      <td>3640</td>\n      <td>0.256200</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>0.263100</td>\n    </tr>\n    <tr>\n      <td>3660</td>\n      <td>0.269100</td>\n    </tr>\n    <tr>\n      <td>3670</td>\n      <td>0.283300</td>\n    </tr>\n    <tr>\n      <td>3680</td>\n      <td>0.272100</td>\n    </tr>\n    <tr>\n      <td>3690</td>\n      <td>0.241500</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.238100</td>\n    </tr>\n    <tr>\n      <td>3710</td>\n      <td>0.245500</td>\n    </tr>\n    <tr>\n      <td>3720</td>\n      <td>0.291300</td>\n    </tr>\n    <tr>\n      <td>3730</td>\n      <td>0.330300</td>\n    </tr>\n    <tr>\n      <td>3740</td>\n      <td>0.283500</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>0.244000</td>\n    </tr>\n    <tr>\n      <td>3760</td>\n      <td>0.303300</td>\n    </tr>\n    <tr>\n      <td>3770</td>\n      <td>0.252000</td>\n    </tr>\n    <tr>\n      <td>3780</td>\n      <td>0.256300</td>\n    </tr>\n    <tr>\n      <td>3790</td>\n      <td>0.274800</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.301800</td>\n    </tr>\n    <tr>\n      <td>3810</td>\n      <td>0.296500</td>\n    </tr>\n    <tr>\n      <td>3820</td>\n      <td>0.305800</td>\n    </tr>\n    <tr>\n      <td>3830</td>\n      <td>0.283200</td>\n    </tr>\n    <tr>\n      <td>3840</td>\n      <td>0.298700</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>0.296200</td>\n    </tr>\n    <tr>\n      <td>3860</td>\n      <td>0.274200</td>\n    </tr>\n    <tr>\n      <td>3870</td>\n      <td>0.329800</td>\n    </tr>\n    <tr>\n      <td>3880</td>\n      <td>0.247000</td>\n    </tr>\n    <tr>\n      <td>3890</td>\n      <td>0.242600</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.263100</td>\n    </tr>\n    <tr>\n      <td>3910</td>\n      <td>0.283500</td>\n    </tr>\n    <tr>\n      <td>3920</td>\n      <td>0.272800</td>\n    </tr>\n    <tr>\n      <td>3930</td>\n      <td>0.273800</td>\n    </tr>\n    <tr>\n      <td>3940</td>\n      <td>0.257600</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>0.228200</td>\n    </tr>\n    <tr>\n      <td>3960</td>\n      <td>0.315700</td>\n    </tr>\n    <tr>\n      <td>3970</td>\n      <td>0.259000</td>\n    </tr>\n    <tr>\n      <td>3980</td>\n      <td>0.304500</td>\n    </tr>\n    <tr>\n      <td>3990</td>\n      <td>0.254500</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.273300</td>\n    </tr>\n    <tr>\n      <td>4010</td>\n      <td>0.293100</td>\n    </tr>\n    <tr>\n      <td>4020</td>\n      <td>0.251700</td>\n    </tr>\n    <tr>\n      <td>4030</td>\n      <td>0.293100</td>\n    </tr>\n    <tr>\n      <td>4040</td>\n      <td>0.316300</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>0.234200</td>\n    </tr>\n    <tr>\n      <td>4060</td>\n      <td>0.305200</td>\n    </tr>\n    <tr>\n      <td>4070</td>\n      <td>0.286300</td>\n    </tr>\n    <tr>\n      <td>4080</td>\n      <td>0.228500</td>\n    </tr>\n    <tr>\n      <td>4090</td>\n      <td>0.269700</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.231900</td>\n    </tr>\n    <tr>\n      <td>4110</td>\n      <td>0.260900</td>\n    </tr>\n    <tr>\n      <td>4120</td>\n      <td>0.304900</td>\n    </tr>\n    <tr>\n      <td>4130</td>\n      <td>0.257900</td>\n    </tr>\n    <tr>\n      <td>4140</td>\n      <td>0.232000</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>0.293700</td>\n    </tr>\n    <tr>\n      <td>4160</td>\n      <td>0.298500</td>\n    </tr>\n    <tr>\n      <td>4170</td>\n      <td>0.265000</td>\n    </tr>\n    <tr>\n      <td>4180</td>\n      <td>0.310700</td>\n    </tr>\n    <tr>\n      <td>4190</td>\n      <td>0.239500</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.228000</td>\n    </tr>\n    <tr>\n      <td>4210</td>\n      <td>0.281700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4210, training_loss=0.33011943461492904, metrics={'train_runtime': 6182.0265, 'train_samples_per_second': 21.789, 'train_steps_per_second': 0.681, 'total_flos': 3.556320164917248e+16, 'train_loss': 0.33011943461492904, 'epoch': 2.0})"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"**Performance Evaluation**","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\nfrom peft import LoraConfig, get_peft_model\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T13:36:06.659262Z","iopub.execute_input":"2025-04-18T13:36:06.659565Z","iopub.status.idle":"2025-04-18T13:36:36.466726Z","shell.execute_reply.started":"2025-04-18T13:36:06.659543Z","shell.execute_reply":"2025-04-18T13:36:36.465583Z"}},"outputs":[{"name":"stderr","text":"2025-04-18 13:36:20.032020: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744983380.342523      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744983380.419590      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Evaluate the model on the validation set\neval_results = trainer.evaluate()\n\n# Print evaluation metrics (e.g., accuracy, loss)\nprint(f\"Validation Loss: {eval_results['eval_loss']}\")\nprint(f\"Validation Accuracy: {eval_results['eval_accuracy']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:13:03.914688Z","iopub.execute_input":"2025-04-18T16:13:03.915303Z","iopub.status.idle":"2025-04-18T16:13:25.041070Z","shell.execute_reply.started":"2025-04-18T16:13:03.915280Z","shell.execute_reply":"2025-04-18T16:13:25.040498Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [28/28 00:20]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Validation Loss: 0.2664070427417755\nValidation Accuracy: 0.8910550458715596\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"**Comparison with BERT Base Model**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n    acc = accuracy_score(labels, predictions)\n    return {\"accuracy\": acc}\n\n# Load regular BERT model (without LoRA)\nbase_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=1,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    do_train=True,\n    do_eval=True,\n    eval_steps=500,  # Evaluate after every 500 steps\n)\n\n# Set up Trainer for base model\ntrainer_base = Trainer(\n    model=base_model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\n\n# Fine-tune the base model\ntrainer_base.train()\n\n# Evaluate the base model\neval_results_base = trainer_base.evaluate()\n\n# Print evaluation metrics for base model\nprint(f\"Base Model Validation Loss: {eval_results_base['eval_loss']}\")\nprint(f\"Base Model Validation Accuracy: {eval_results_base['eval_accuracy']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T16:49:41.779239Z","iopub.execute_input":"2025-04-18T16:49:41.779545Z","iopub.status.idle":"2025-04-18T17:54:20.023999Z","shell.execute_reply.started":"2025-04-18T16:49:41.779525Z","shell.execute_reply":"2025-04-18T17:54:20.023419Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_31/2449771275.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer_base = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2105' max='2105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2105/2105 1:04:16, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.711400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.642500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.545600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.452400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.356400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.367400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.352300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.303400</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.244700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.419100</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.310300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.312300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.258100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.280700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.280800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.313100</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.303900</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.277100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.269400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.268900</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.303500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.273000</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.297100</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.286400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.338900</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.253200</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.289500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.311200</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.298500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.280500</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.278000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.246700</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.230700</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.217600</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.240000</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.228200</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.264400</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.209400</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.265500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.243900</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.244700</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.271800</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.273400</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.234600</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.272300</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.215700</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.260100</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.189900</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.223400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.233600</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>0.207100</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.184100</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>0.260000</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.230000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.243900</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.218900</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>0.192200</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.247700</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>0.207300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.218600</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>0.239500</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.199000</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.220900</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.243300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.199700</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>0.222900</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.189300</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>0.188000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.223800</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>0.262000</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.205700</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>0.143500</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.319200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.180700</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.221900</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>0.207700</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.201300</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>0.203300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.189900</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>0.196500</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.190400</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>0.247800</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.177900</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.169500</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.144000</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>0.220800</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.183000</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>0.176700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.202300</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>0.197300</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.241000</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>0.163200</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.171700</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.187700</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.229300</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>0.180500</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.190900</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>0.185600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.172900</td>\n    </tr>\n    <tr>\n      <td>1010</td>\n      <td>0.259500</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.209000</td>\n    </tr>\n    <tr>\n      <td>1030</td>\n      <td>0.190700</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.185200</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.200700</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.146300</td>\n    </tr>\n    <tr>\n      <td>1070</td>\n      <td>0.181800</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.172200</td>\n    </tr>\n    <tr>\n      <td>1090</td>\n      <td>0.124500</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.214800</td>\n    </tr>\n    <tr>\n      <td>1110</td>\n      <td>0.167200</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.182900</td>\n    </tr>\n    <tr>\n      <td>1130</td>\n      <td>0.124500</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.196600</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.119700</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.166400</td>\n    </tr>\n    <tr>\n      <td>1170</td>\n      <td>0.170300</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.157800</td>\n    </tr>\n    <tr>\n      <td>1190</td>\n      <td>0.129400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.205600</td>\n    </tr>\n    <tr>\n      <td>1210</td>\n      <td>0.165000</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.143300</td>\n    </tr>\n    <tr>\n      <td>1230</td>\n      <td>0.224600</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.216400</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.150200</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.161000</td>\n    </tr>\n    <tr>\n      <td>1270</td>\n      <td>0.187100</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.197800</td>\n    </tr>\n    <tr>\n      <td>1290</td>\n      <td>0.131200</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.198400</td>\n    </tr>\n    <tr>\n      <td>1310</td>\n      <td>0.208700</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.205100</td>\n    </tr>\n    <tr>\n      <td>1330</td>\n      <td>0.231700</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.145500</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.126200</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.221900</td>\n    </tr>\n    <tr>\n      <td>1370</td>\n      <td>0.126000</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.229400</td>\n    </tr>\n    <tr>\n      <td>1390</td>\n      <td>0.122600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.164400</td>\n    </tr>\n    <tr>\n      <td>1410</td>\n      <td>0.188100</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.193900</td>\n    </tr>\n    <tr>\n      <td>1430</td>\n      <td>0.163800</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.171100</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.181900</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.234900</td>\n    </tr>\n    <tr>\n      <td>1470</td>\n      <td>0.178000</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.141100</td>\n    </tr>\n    <tr>\n      <td>1490</td>\n      <td>0.159100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.217000</td>\n    </tr>\n    <tr>\n      <td>1510</td>\n      <td>0.146800</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.103100</td>\n    </tr>\n    <tr>\n      <td>1530</td>\n      <td>0.168000</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.126700</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>0.191700</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.161000</td>\n    </tr>\n    <tr>\n      <td>1570</td>\n      <td>0.207300</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.225100</td>\n    </tr>\n    <tr>\n      <td>1590</td>\n      <td>0.177200</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.203000</td>\n    </tr>\n    <tr>\n      <td>1610</td>\n      <td>0.135000</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.206500</td>\n    </tr>\n    <tr>\n      <td>1630</td>\n      <td>0.125600</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.159200</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>0.151400</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.187900</td>\n    </tr>\n    <tr>\n      <td>1670</td>\n      <td>0.170400</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.187800</td>\n    </tr>\n    <tr>\n      <td>1690</td>\n      <td>0.195400</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.199200</td>\n    </tr>\n    <tr>\n      <td>1710</td>\n      <td>0.158100</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.177200</td>\n    </tr>\n    <tr>\n      <td>1730</td>\n      <td>0.217800</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.148400</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>0.124900</td>\n    </tr>\n    <tr>\n      <td>1760</td>\n      <td>0.170100</td>\n    </tr>\n    <tr>\n      <td>1770</td>\n      <td>0.192100</td>\n    </tr>\n    <tr>\n      <td>1780</td>\n      <td>0.128400</td>\n    </tr>\n    <tr>\n      <td>1790</td>\n      <td>0.151400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.159300</td>\n    </tr>\n    <tr>\n      <td>1810</td>\n      <td>0.191400</td>\n    </tr>\n    <tr>\n      <td>1820</td>\n      <td>0.153400</td>\n    </tr>\n    <tr>\n      <td>1830</td>\n      <td>0.151000</td>\n    </tr>\n    <tr>\n      <td>1840</td>\n      <td>0.171900</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>0.163800</td>\n    </tr>\n    <tr>\n      <td>1860</td>\n      <td>0.174400</td>\n    </tr>\n    <tr>\n      <td>1870</td>\n      <td>0.119900</td>\n    </tr>\n    <tr>\n      <td>1880</td>\n      <td>0.150500</td>\n    </tr>\n    <tr>\n      <td>1890</td>\n      <td>0.186800</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.146600</td>\n    </tr>\n    <tr>\n      <td>1910</td>\n      <td>0.199600</td>\n    </tr>\n    <tr>\n      <td>1920</td>\n      <td>0.148100</td>\n    </tr>\n    <tr>\n      <td>1930</td>\n      <td>0.143200</td>\n    </tr>\n    <tr>\n      <td>1940</td>\n      <td>0.144800</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>0.112600</td>\n    </tr>\n    <tr>\n      <td>1960</td>\n      <td>0.177700</td>\n    </tr>\n    <tr>\n      <td>1970</td>\n      <td>0.149200</td>\n    </tr>\n    <tr>\n      <td>1980</td>\n      <td>0.159500</td>\n    </tr>\n    <tr>\n      <td>1990</td>\n      <td>0.162000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.159500</td>\n    </tr>\n    <tr>\n      <td>2010</td>\n      <td>0.133300</td>\n    </tr>\n    <tr>\n      <td>2020</td>\n      <td>0.135600</td>\n    </tr>\n    <tr>\n      <td>2030</td>\n      <td>0.186400</td>\n    </tr>\n    <tr>\n      <td>2040</td>\n      <td>0.144100</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>0.146200</td>\n    </tr>\n    <tr>\n      <td>2060</td>\n      <td>0.153800</td>\n    </tr>\n    <tr>\n      <td>2070</td>\n      <td>0.139400</td>\n    </tr>\n    <tr>\n      <td>2080</td>\n      <td>0.102600</td>\n    </tr>\n    <tr>\n      <td>2090</td>\n      <td>0.159700</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.191400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [28/28 00:17]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Base Model Validation Loss: 0.2329603135585785\nBase Model Validation Accuracy: 0.9151376146788991\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"**Performance Comparison**","metadata":{}},{"cell_type":"code","source":"# Compare evaluation results\nprint(f\"LoRA Model - Validation Loss: {eval_results['eval_loss']}, Accuracy: {eval_results['eval_accuracy']}\")\nprint(f\"Base Model - Validation Loss: {eval_results_base['eval_loss']}, Accuracy: {eval_results_base['eval_accuracy']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T17:54:30.646703Z","iopub.execute_input":"2025-04-18T17:54:30.647422Z","iopub.status.idle":"2025-04-18T17:54:30.655789Z","shell.execute_reply.started":"2025-04-18T17:54:30.647384Z","shell.execute_reply":"2025-04-18T17:54:30.655058Z"}},"outputs":[{"name":"stdout","text":"LoRA Model - Validation Loss: 0.2664070427417755, Accuracy: 0.8910550458715596\nBase Model - Validation Loss: 0.2329603135585785, Accuracy: 0.9151376146788991\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}