{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdgVeaWMV4cT",
        "outputId": "ecf6d9d8-6e60-469f-863f-ecf77186641e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**QUESTION 4**\n",
        "\n",
        "For the RNN-based translation model, Train the model twice:\n",
        "1. Using randomly initialized embeddings for the English input.\n",
        "2. Using pre-trained GloVe embeddings for the English input.\n",
        "https://nlp.stanford.edu/projects/glove/\n",
        "3. Compare the results between the two versions.\n",
        "4. Discuss the impact of using pre-trained embeddings on model performance and\n",
        "training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITs_OJGoWMU5",
        "outputId": "2a608ade-07d1-4bbf-d4e6-66ba6ecf24f3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install -q nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, LSTM, Dense, Bidirectional, GRU, MultiHeadAttention, LayerNormalization, Dropout, Add\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mws6G1zIZTHv"
      },
      "outputs": [],
      "source": [
        "# Load the files\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/english-corpus.txt', 'r', encoding='utf-8') as f:\n",
        "    english_lines = f.read().splitlines()\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/urdu-corpus.txt', 'r', encoding='utf-8') as f:\n",
        "    urdu_lines = f.read().splitlines()\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({'english': english_lines, 'urdu': urdu_lines}).dropna()\n",
        "\n",
        "# Add <sos> and <eos> tokens to Urdu (target)\n",
        "df['urdu'] = df['urdu'].apply(lambda x: '<sos> ' + x + ' <eos>')\n",
        "\n",
        "# Tokenization\n",
        "eng_tokenizer = Tokenizer(oov_token='<OOV>', filters='')\n",
        "urdu_tokenizer = Tokenizer(oov_token='<OOV>', filters='')\n",
        "\n",
        "eng_tokenizer.fit_on_texts(df['english'])\n",
        "urdu_tokenizer.fit_on_texts(df['urdu'])\n",
        "\n",
        "# Convert to sequences\n",
        "input_seq = eng_tokenizer.texts_to_sequences(df['english'])\n",
        "target_seq = urdu_tokenizer.texts_to_sequences(df['urdu'])\n",
        "\n",
        "\n",
        "# Padding lengths\n",
        "max_input_len = max(len(seq) for seq in input_seq)\n",
        "max_target_len = max(len(seq) for seq in target_seq)\n",
        "\n",
        "# Pad encoder input\n",
        "encoder_input = pad_sequences(input_seq, maxlen=max_input_len, padding='post')\n",
        "\n",
        "# New decoder input/target processing\n",
        "def preprocess_decoder_data(tokenized_urdu):\n",
        "    decoder_input = []\n",
        "    decoder_target = []\n",
        "\n",
        "    for seq in tokenized_urdu:\n",
        "        inp = seq[:-1]  # remove <eos>\n",
        "        tar = seq[1:]   # remove <sos>\n",
        "        decoder_input.append(inp + [0] * (max_target_len - len(inp)))\n",
        "        decoder_target.append(tar + [0] * (max_target_len - len(tar)))\n",
        "\n",
        "    return np.array(decoder_input), np.array(decoder_target)\n",
        "\n",
        "# Apply preprocessing\n",
        "decoder_input, decoder_target = preprocess_decoder_data(target_seq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DfWGT-HCZU-j"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE_EN = len(eng_tokenizer.word_index) + 1\n",
        "VOCAB_SIZE_UR = len(urdu_tokenizer.word_index) + 1\n",
        "EMBEDDING_DIM = 300\n",
        "UNITS = 512\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "g8Q0Mf1DZskE"
      },
      "outputs": [],
      "source": [
        "def train_model(model, model_name):\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(\n",
        "        [encoder_input, decoder_input], decoder_target,\n",
        "        batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "        validation_split=0.2\n",
        "    )\n",
        "    model.save(f\"{model_name}_model.h5\")\n",
        "\n",
        "def decode_sequence_greedy(model, input_seq):\n",
        "    target_seq = np.zeros((1, max_target_len))\n",
        "    target_seq[0, 0] = urdu_tokenizer.word_index['<sos>']\n",
        "\n",
        "    decoded_sentence = []\n",
        "    for i in range(1, max_target_len):\n",
        "        output_tokens = model.predict([input_seq, target_seq], verbose=0)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, i-1, :])\n",
        "        sampled_word = urdu_tokenizer.index_word.get(sampled_token_index, '')\n",
        "        if sampled_word == '<eos>' or sampled_word == '':\n",
        "            break\n",
        "        decoded_sentence.append(sampled_word)\n",
        "        target_seq[0, i] = sampled_token_index\n",
        "    return ' '.join(decoded_sentence)\n",
        "\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "smoothie = SmoothingFunction().method4\n",
        "\n",
        "def evaluate_bleu(model, sample_indices):\n",
        "    bleu_scores = []\n",
        "\n",
        "    for idx in sample_indices:\n",
        "        input_seq = encoder_input[idx:idx+1]\n",
        "        true_output = df['urdu'].iloc[idx].replace('<sos>', '').replace('<eos>', '').strip()\n",
        "        pred_text = decode_sequence_greedy(model, input_seq)\n",
        "\n",
        "        bleu = sentence_bleu([true_output.split()], pred_text.split(), smoothing_function=smoothie)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        print(f\"\\nğŸ”¹Input: {df['english'].iloc[idx]}\")\n",
        "        print(f\"ğŸ”¹Target: {true_output}\")\n",
        "        print(f\"ğŸ”¹Predicted: {pred_text}\")\n",
        "        print(f\"ğŸ”¹BLEU Score: {bleu:.4f}\")\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    print(f\"\\nâœ¨ Average BLEU Score: {avg_bleu:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qArEqLifZuGs"
      },
      "outputs": [],
      "source": [
        "def load_glove_embeddings(glove_path, embedding_dim):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "glove_path = '/content/drive/MyDrive/Colab Notebooks/glove.6B.300d.txt'\n",
        "EMBEDDING_DIM = 300  # Update global EMBEDDING_DIM if needed\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(glove_path, EMBEDDING_DIM)\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE_EN, EMBEDDING_DIM))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GreW-AaiZwou"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.zeros((VOCAB_SIZE_EN, 300))  # 100 for glove.27B.100d\n",
        "\n",
        "for word, i in eng_tokenizer.word_index.items():\n",
        "    embedding_vector = glove_embeddings.get(word.lower())\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qi8kGw7yZyLm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, SimpleRNN, Bidirectional, Dense, Concatenate, Attention, AdditiveAttention\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_seq2seq_with_attention(pretrained_embedding=None):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_input_len,))\n",
        "\n",
        "    if pretrained_embedding is not None:\n",
        "        encoder_embedding = Embedding(\n",
        "            input_dim=VOCAB_SIZE_EN,\n",
        "            output_dim=pretrained_embedding.shape[1],\n",
        "            weights=[pretrained_embedding],\n",
        "            input_length=max_input_len,\n",
        "            trainable=True\n",
        "        )(encoder_inputs)\n",
        "    else:\n",
        "        encoder_embedding = Embedding(VOCAB_SIZE_EN, EMBEDDING_DIM)(encoder_inputs)\n",
        "\n",
        "    # Encoder LSTM\n",
        "    encoder_outputs, state_h, state_c = LSTM(UNITS, return_sequences=True, return_state=True)(encoder_embedding)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(max_target_len,))\n",
        "    decoder_embedding = Embedding(VOCAB_SIZE_UR, EMBEDDING_DIM)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(UNITS, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
        "\n",
        "    # Attention\n",
        "    attention = AdditiveAttention()\n",
        "    attention_result = attention([decoder_outputs, encoder_outputs])  # context vector\n",
        "\n",
        "    # Concatenate context with decoder outputs\n",
        "    concat = Concatenate(axis=-1)([decoder_outputs, attention_result])\n",
        "\n",
        "    # Final Dense layer\n",
        "    dense = Dense(VOCAB_SIZE_UR, activation='softmax')\n",
        "    output = dense(concat)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], output)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orFWCG6eZ3oP",
        "outputId": "41bc8c0c-48c7-4b2b-f4fc-326d0e1c1012"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 81ms/step - accuracy: 0.7485 - loss: 2.0060 - val_accuracy: 0.8169 - val_loss: 1.1126\n",
            "Epoch 2/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 76ms/step - accuracy: 0.8315 - loss: 0.9836 - val_accuracy: 0.8581 - val_loss: 0.8076\n",
            "Epoch 3/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.8708 - loss: 0.6530 - val_accuracy: 0.8823 - val_loss: 0.6387\n",
            "Epoch 4/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 76ms/step - accuracy: 0.9039 - loss: 0.4195 - val_accuracy: 0.8958 - val_loss: 0.5585\n",
            "Epoch 5/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.9315 - loss: 0.2694 - val_accuracy: 0.9023 - val_loss: 0.5297\n",
            "Epoch 6/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 76ms/step - accuracy: 0.9520 - loss: 0.1802 - val_accuracy: 0.9065 - val_loss: 0.5154\n",
            "Epoch 7/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.9659 - loss: 0.1283 - val_accuracy: 0.9083 - val_loss: 0.5173\n",
            "Epoch 8/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 75ms/step - accuracy: 0.9757 - loss: 0.0947 - val_accuracy: 0.9104 - val_loss: 0.5207\n",
            "Epoch 9/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - accuracy: 0.9830 - loss: 0.0686 - val_accuracy: 0.9109 - val_loss: 0.5284\n",
            "Epoch 10/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9887 - loss: 0.0492 - val_accuracy: 0.9114 - val_loss: 0.5356\n",
            "Epoch 11/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 75ms/step - accuracy: 0.9925 - loss: 0.0366 - val_accuracy: 0.9118 - val_loss: 0.5431\n",
            "Epoch 12/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 78ms/step - accuracy: 0.9946 - loss: 0.0276 - val_accuracy: 0.9126 - val_loss: 0.5536\n",
            "Epoch 13/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9964 - loss: 0.0207 - val_accuracy: 0.9125 - val_loss: 0.5675\n",
            "Epoch 14/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 75ms/step - accuracy: 0.9974 - loss: 0.0163 - val_accuracy: 0.9140 - val_loss: 0.5701\n",
            "Epoch 15/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 78ms/step - accuracy: 0.9981 - loss: 0.0127 - val_accuracy: 0.9136 - val_loss: 0.5802\n",
            "Epoch 16/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - accuracy: 0.9987 - loss: 0.0098 - val_accuracy: 0.9140 - val_loss: 0.5885\n",
            "Epoch 17/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 75ms/step - accuracy: 0.9988 - loss: 0.0086 - val_accuracy: 0.9138 - val_loss: 0.6003\n",
            "Epoch 18/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - accuracy: 0.9990 - loss: 0.0076 - val_accuracy: 0.9137 - val_loss: 0.6024\n",
            "Epoch 19/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.9986 - loss: 0.0090 - val_accuracy: 0.9126 - val_loss: 0.6192\n",
            "Epoch 20/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 75ms/step - accuracy: 0.9976 - loss: 0.0123 - val_accuracy: 0.9107 - val_loss: 0.6359\n",
            "Epoch 21/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.9962 - loss: 0.0165 - val_accuracy: 0.9122 - val_loss: 0.6303\n",
            "Epoch 22/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 78ms/step - accuracy: 0.9967 - loss: 0.0145 - val_accuracy: 0.9131 - val_loss: 0.6419\n",
            "Epoch 23/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - accuracy: 0.9978 - loss: 0.0112 - val_accuracy: 0.9133 - val_loss: 0.6449\n",
            "Epoch 24/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9987 - loss: 0.0071 - val_accuracy: 0.9135 - val_loss: 0.6569\n",
            "Epoch 25/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 75ms/step - accuracy: 0.9991 - loss: 0.0053 - val_accuracy: 0.9135 - val_loss: 0.6573\n",
            "Epoch 26/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 78ms/step - accuracy: 0.9994 - loss: 0.0039 - val_accuracy: 0.9144 - val_loss: 0.6658\n",
            "Epoch 27/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 75ms/step - accuracy: 0.9994 - loss: 0.0041 - val_accuracy: 0.9147 - val_loss: 0.6696\n",
            "Epoch 28/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 78ms/step - accuracy: 0.9995 - loss: 0.0035 - val_accuracy: 0.9142 - val_loss: 0.6659\n",
            "Epoch 29/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - accuracy: 0.9991 - loss: 0.0048 - val_accuracy: 0.9115 - val_loss: 0.6876\n",
            "Epoch 30/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 75ms/step - accuracy: 0.9964 - loss: 0.0133 - val_accuracy: 0.9090 - val_loss: 0.7102\n",
            "Epoch 31/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.9954 - loss: 0.0171 - val_accuracy: 0.9111 - val_loss: 0.6987\n",
            "Epoch 32/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 76ms/step - accuracy: 0.9970 - loss: 0.0119 - val_accuracy: 0.9121 - val_loss: 0.7056\n",
            "Epoch 33/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - accuracy: 0.9988 - loss: 0.0058 - val_accuracy: 0.9134 - val_loss: 0.7074\n",
            "Epoch 34/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 76ms/step - accuracy: 0.9993 - loss: 0.0035 - val_accuracy: 0.9139 - val_loss: 0.7083\n",
            "Epoch 35/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - accuracy: 0.9995 - loss: 0.0029 - val_accuracy: 0.9143 - val_loss: 0.7073\n",
            "Epoch 36/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 76ms/step - accuracy: 0.9996 - loss: 0.0022 - val_accuracy: 0.9150 - val_loss: 0.7103\n",
            "Epoch 37/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.9996 - loss: 0.0022 - val_accuracy: 0.9148 - val_loss: 0.7112\n",
            "Epoch 38/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 79ms/step - accuracy: 0.9997 - loss: 0.0020 - val_accuracy: 0.9149 - val_loss: 0.7140\n",
            "Epoch 39/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9997 - loss: 0.0019 - val_accuracy: 0.9148 - val_loss: 0.7167\n",
            "Epoch 40/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.9996 - loss: 0.0021 - val_accuracy: 0.9136 - val_loss: 0.7184\n",
            "Epoch 41/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - accuracy: 0.9981 - loss: 0.0073 - val_accuracy: 0.9066 - val_loss: 0.7469\n",
            "Epoch 42/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 76ms/step - accuracy: 0.9925 - loss: 0.0252 - val_accuracy: 0.9107 - val_loss: 0.7287\n",
            "Epoch 43/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 76ms/step - accuracy: 0.9970 - loss: 0.0111 - val_accuracy: 0.9118 - val_loss: 0.7421\n",
            "Epoch 44/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 78ms/step - accuracy: 0.9987 - loss: 0.0059 - val_accuracy: 0.9134 - val_loss: 0.7377\n",
            "Epoch 45/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9995 - loss: 0.0029 - val_accuracy: 0.9140 - val_loss: 0.7335\n",
            "Epoch 46/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 78ms/step - accuracy: 0.9996 - loss: 0.0019 - val_accuracy: 0.9143 - val_loss: 0.7392\n",
            "Epoch 47/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 77ms/step - accuracy: 0.9997 - loss: 0.0017 - val_accuracy: 0.9146 - val_loss: 0.7391\n",
            "Epoch 48/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 76ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.9151 - val_loss: 0.7376\n",
            "Epoch 49/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 76ms/step - accuracy: 0.9997 - loss: 0.0017 - val_accuracy: 0.9151 - val_loss: 0.7449\n",
            "Epoch 50/50\n",
            "\u001b[1m307/307\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 75ms/step - accuracy: 0.9997 - loss: 0.0016 - val_accuracy: 0.9148 - val_loss: 0.7449\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ”¹Input: zain was hesitant\n",
            "ğŸ”¹Target: Ø²ÛŒÙ† ÛÚ†Ú©Ú†Ø§ Ø±ÛØ§ ØªÚ¾Ø§\n",
            "ğŸ”¹Predicted: Ø²ÛŒÙ† ÛÚ†Ú©Ú†Ø§ Ø±ÛØ§ ØªÚ¾Ø§\n",
            "ğŸ”¹BLEU Score: 1.0000\n",
            "\n",
            "ğŸ”¹Input: did zain give you that\n",
            "ğŸ”¹Target: Ø²ÛŒÙ† Ù†Û’ ØªÙ…ÛÛŒÚº ÙˆÛ Ø¯ÛŒØ§\n",
            "ğŸ”¹Predicted: Ø²ÛŒÙ† Ù†Û’ ØªÙ…ÛÛŒÚº ÙˆÛ Ø¯ÛŒØ§\n",
            "ğŸ”¹BLEU Score: 1.0000\n",
            "\n",
            "ğŸ”¹Input: i come from china\n",
            "ğŸ”¹Target: Ù…ÛŒÚº Ú†ÛŒÙ† Ø³Û’ Ø¢ÛŒØ§ ÛÙˆÚºÛ”\n",
            "ğŸ”¹Predicted: Ù…ÛŒÚº Ú†ÛŒÙ† Ø³Û’ Ø¢ÛŒØ§ ÛÙˆÚºÛ”\n",
            "ğŸ”¹BLEU Score: 1.0000\n",
            "\n",
            "âœ¨ Average BLEU Score: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# GloVe: Pretrained Embeddings\n",
        "rnn_model_glove = build_seq2seq_with_attention(pretrained_embedding=embedding_matrix)\n",
        "train_model(rnn_model_glove, 'rnn_glove')\n",
        "evaluate_bleu(rnn_model_glove, [10, 20, 30])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
